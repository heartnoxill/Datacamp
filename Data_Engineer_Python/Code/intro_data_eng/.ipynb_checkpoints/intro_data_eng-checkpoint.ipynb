{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engineering toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database schema\n",
    "\n",
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())\n",
    "\n",
    "\"\"\"\n",
    "      first_name last_name\n",
    "    0    Connagh    Bailey\n",
    "    1      Brook     Bloom\n",
    "    2        Ann    Dalton\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 7 entries, 0 to 6\n",
    "    Data columns (total 2 columns):\n",
    "    first_name    7 non-null object\n",
    "    last_name     7 non-null object\n",
    "    dtypes: object(2)\n",
    "    memory usage: 192.0+ bytes\n",
    "    None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining on relations\n",
    "\n",
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From task to subtasks\n",
    "\n",
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)\n",
    "\n",
    "\"\"\"\n",
    "    Processing time: 1601.813554763794\n",
    "    Processing time: 1104.118824005127\n",
    "    Processing time: 1199.4798183441162\n",
    "    \n",
    "    In reality, using parallel computing wouldn't make sense here since the computations are simple and the dataset is \n",
    "    relatively small. Communication overhead costs the multiprocessing version its edge!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a DataFrame\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of pratitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PySpark groupby\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running PySpark files\n",
    "\n",
    "\"\"\"cat /home/repl/spark-script.py\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    athlete_events_spark = (spark\n",
    "        .read\n",
    "        .csv(\"/home/repl/datasets/athlete_events.csv\",\n",
    "             header=True,\n",
    "             inferSchema=True,\n",
    "             escape='\"'))\n",
    "\n",
    "    athlete_events_spark = (athlete_events_spark\n",
    "        .withColumn(\"Height\",\n",
    "                    athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "    print(athlete_events_spark\n",
    "        .groupBy('Year')\n",
    "        .mean('Height')\n",
    "        .orderBy('Year')\n",
    "        .show())\n",
    "    \n",
    "\"\"\"spark-submit --master local[4] /home/repl/spark-script.py\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Airflow, Luigi and cron\n",
    "In the video, you saw several tools that can help you with the scheduling of your Spark jobs. \n",
    "You saw the limitations of cron and how it has led to the development of frameworks like Airflow and Luigi.\n",
    "\n",
    "There's a lot of useful features in Airflow, but can you select the feature from the list below which is also provided by cron?\n",
    "\n",
    "--> You have exact control over the time at which jobs run.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAGs\n",
    "\n",
    "# https://assets.datacamp.com/production/repositories/5000/datasets/44f52c1b25308c762f24dcde116b62e275ce7fe1/DAG.png\n",
    "\n",
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)\n",
    "\n",
    "# Look at one block at choose \"next\" corresponding to the arrow direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform and Load (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data sources\n",
    "In the previous video you've learned about three ways of extracting data:\n",
    "\n",
    "Extract from text files, like .txt or .csv\n",
    "Extract from APIs of web services, like the Hacker News API\n",
    "Extract from a database, like a SQL application database for customer data\n",
    "We also briefly touched upon row-oriented databases and OLTP.\n",
    "\n",
    "Can you select the statement about these topics which is not true?\n",
    "\n",
    "--> APIs mostly use raw text to transfer data.\n",
    "Note: APIs mostly use a more structured form of data, for example JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch from an API\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a database\n",
    "\n",
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the rental rate\n",
    "\n",
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(str)\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split('.', expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar=rental_rate_expanded[0],\n",
    "    rental_rate_cents=rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare for transformations\n",
    "As mentioned in the video, before you can do transformations using PySpark, you need to get the data into the Spark framework. You saw how to do this using PySpark. Can you choose the correct code?\n",
    "\n",
    "(A)\n",
    "\n",
    "spark.read.jdbc(\"jdbc:postgresql://repl:password@localhost:5432/pagila\",\n",
    "                \"customer\")\n",
    "(B)\n",
    "\n",
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\",\n",
    "                {\"user\":\"repl\",\"password\":\"password\"})\n",
    "(C)\n",
    "\n",
    "spark.read.jdbc(\"jdbc:postgresql://repl:password@localhost:5432/pagila/customer\")\n",
    "\n",
    "--> B\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining with ratings\n",
    "\n",
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OLAP or OLTP\n",
    "You saw that there's a difference between OLAP and OLTP operations. A small recap:\n",
    "\n",
    "OLAP: Online analytical processing\n",
    "OLTP: Online transaction processing\n",
    "It's essential to use the right database for the right job. There's a list of statements below. \n",
    "Can you find the most appropriate statement that is true?\n",
    "\n",
    "--> Typically, analytical databases are column-oriented.\n",
    "\n",
    "Massively parallel processing (MPP) databases are usually column-oriented.\n",
    "\n",
    "Databases optimized for OLAP are usually not great at OLTP operations.\n",
    "\n",
    "Analytical and application databases have different use cases and should be separated if possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a file\n",
    "\n",
    "# Write the pandas DataFrame to parquet\n",
    "film_pdf.to_parquet(\"films_pdf.parquet\")\n",
    "\n",
    "# Write the PySpark DataFrame to parquet\n",
    "film_sdf.write.parquet(\"films_sdf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into Postgres\n",
    "\n",
    "# Finish the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation step, join with recommendations data\n",
    "film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "film_pdf_joined.to_sql(\"film\", db_engine_dwh, schema=\"store\", if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a DAG\n",
    "\n",
    "# Define the ETL function\n",
    "def etl():\n",
    "    film_df = extract_film_to_pandas()\n",
    "    film_df = transform_rental_rate(film_df)\n",
    "    load_dataframe_to_film(film_df)\n",
    "\n",
    "# Define the ETL task using PythonOperator\n",
    "etl_task = PythonOperator(task_id='etl_film',\n",
    "                          python_callable=etl,\n",
    "                          dag=dag)\n",
    "\n",
    "# Set the upstream to wait_for_table and sample run etl()\n",
    "etl_task.set_upstream(wait_for_table)\n",
    "etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Airflow\n",
    "\n",
    "\"\"\"\n",
    "The airflow home directory is defined in the AIRFLOW_HOME environment variable. Type echo $AIRFLOW_HOME to find out.\n",
    "\"~/airflow\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the table\n",
    "\n",
    "# Complete the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/datacamp_application\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Get user with id 4387\n",
    "user1 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=4387\", db_engine)\n",
    "\n",
    "# Get user with id 18163\n",
    "user2 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=18163\", db_engine)\n",
    "\n",
    "# Get user with id 8770\n",
    "user3 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=8770\", db_engine)\n",
    "\n",
    "# Use the helper function to compare the 3 users\n",
    "print_user_comparison(user1, user2, user3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating per course\n",
    "\n",
    "# Complete the transformation function\n",
    "def transform_avg_rating(rating_data):\n",
    "  # Group by course_id and extract average rating per course\n",
    "  avg_rating = rating_data.groupby('course_id').rating.mean()\n",
    "  # Return sorted average ratings per course\n",
    "  sort_rating = avg_rating.sort_values(ascending=False).reset_index()\n",
    "  return sort_rating\n",
    "\n",
    "# Extract the rating data into a DataFrame    \n",
    "rating_data = extract_rating_data(db_engines)\n",
    "\n",
    "# Use transform_avg_rating on the extracted data and print results\n",
    "avg_rating_data = transform_avg_rating(rating_data)\n",
    "print(avg_rating_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out corrupt data\n",
    "\n",
    "course_data = extract_course_data(db_engines)\n",
    "\n",
    "# Print out the number of missing values per column\n",
    "print(course_data.isnull().sum())\n",
    "\n",
    "# The transformation should fill in the missing values\n",
    "def transform_fill_programming_language(course_data):\n",
    "    imputed = course_data.fillna({\"programming_language\": \"r\"})\n",
    "    return imputed\n",
    "\n",
    "transformed = transform_fill_programming_language(course_data)\n",
    "\n",
    "# Print out the number of missing values per column of transformed\n",
    "print(transformed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the recommender transformation\n",
    "\n",
    "# Complete the transformation function\n",
    "def transform_recommendations(avg_course_ratings, courses_to_recommend):\n",
    "    # Merge both DataFrames\n",
    "    merged = courses_to_recommend.merge(avg_course_ratings)\n",
    "    # Sort values by rating and group by user_id\n",
    "    grouped = merged.sort_values(\"rating\", ascending = False).groupby('user_id') \n",
    "    # Produce the top 3 values and sort by user_id\n",
    "    recommendations = grouped.head(3).sort_values(\"user_id\").reset_index()\n",
    "    final_recommendations = recommendations[[\"user_id\", \"course_id\",\"rating\"]]\n",
    "    # Return final recommendations\n",
    "    return final_recommendations\n",
    "\n",
    "# Use the function with the predefined DataFrame objects\n",
    "recommendations = transform_recommendations(avg_course_ratings, courses_to_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target table\n",
    "\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "def load_to_dwh(recommendations):\n",
    "    recommendations.to_sql(\"recommendations\", db_engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the DAG\n",
    "\n",
    "# Define the DAG so it runs on a daily basis\n",
    "dag = DAG(dag_id=\"recommendations\",\n",
    "          schedule_interval=\"0 0 * * *\")\n",
    "\n",
    "# Make sure `etl()` is called in the operator. Pass the correct kwargs.\n",
    "task_recommendations = PythonOperator(\n",
    "    task_id=\"recommendations_task\",\n",
    "    python_callable=etl,\n",
    "    op_kwargs={\"db_engines\":db_engines},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the recommendations\n",
    "\n",
    "def recommendations_for_user(user_id, threshold=4.5):\n",
    "  # Join with the courses table\n",
    "  query = \"\"\"\n",
    "  SELECT title, rating FROM recommendations\n",
    "    INNER JOIN courses ON courses.course_id = recommendations.course_id\n",
    "    WHERE user_id=%(user_id)s AND rating>%(threshold)s\n",
    "    ORDER BY rating DESC\n",
    "  \"\"\"\n",
    "  # Add the threshold parameter\n",
    "  predictions_df = pd.read_sql(query, db_engine, params = {\"user_id\": user_id, \n",
    "                                                           \"threshold\": threshold})\n",
    "  return predictions_df.title.values\n",
    "\n",
    "# Try the function you created\n",
    "print(recommendations_for_user(12, 4.65))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
