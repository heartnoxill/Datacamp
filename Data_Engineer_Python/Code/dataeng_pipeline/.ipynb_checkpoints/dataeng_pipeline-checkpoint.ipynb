{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with JSON\n",
    "\n",
    "# Import json\n",
    "import json\n",
    "\n",
    "database_address = {\n",
    "  \"host\": \"10.0.0.5\",\n",
    "  \"port\": 8456\n",
    "}\n",
    "\n",
    "# Open the configuration file in writable mode\n",
    "with open(\"database_config.json\", \"w\") as fh:\n",
    "  # Serialize the object in this file handle\n",
    "  json.dump(obj=database_address, fp=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the schema of the data\n",
    "\n",
    "\"\"\"\n",
    "{'items': [{'brand': 'Huggies',\n",
    "            'model': 'newborn',\n",
    "            'price': 6.8,\n",
    "            'currency': 'EUR',            \n",
    "            'quantity': 40,\n",
    "            'date': '2019-02-01',\n",
    "            'countrycode': 'DE'            \n",
    "            },\n",
    "           {…}]\n",
    "\"\"\"\n",
    "\n",
    "# Complete the JSON schema\n",
    "schema = {'properties': {\n",
    "    'brand': {'type': 'string'},\n",
    "    'model': {'type': 'string'},\n",
    "    'price': {'type': 'number'},\n",
    "    'currency': {'type': 'string'},\n",
    "    'quantity': {'type': 'integer', 'minimum': 1},\n",
    "    'date': {'type': 'string', 'format': 'date'}, \n",
    "    'countrycode': {'type': 'string', 'pattern': \"^[A-Z]{2}$\"},\n",
    "    'store_name': {'type': 'string'}}}\n",
    "\n",
    "# Write the schema\n",
    "singer.write_schema(stream_name='products', schema=schema, key_properties=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communicating with an API\n",
    "\n",
    "endpoint = \"http://localhost:5000\"\n",
    "\n",
    "# Fill in the correct API key\n",
    "api_key = \"scientist007\"\n",
    "\n",
    "# Create the web API’s URL\n",
    "authenticated_endpoint = \"{}/{}\".format(endpoint, api_key)\n",
    "\n",
    "# Get the web API’s reply to the endpoint\n",
    "api_response = requests.get(authenticated_endpoint).json()\n",
    "pprint.pprint(api_response)\n",
    "\n",
    "# Create the API’s endpoint for the shops\n",
    "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
    "shops = requests.get(shops_endpoint).json()\n",
    "print(shops)\n",
    "\n",
    "# Create the API’s endpoint for items of the shop starting with a \"D\"\n",
    "items_of_specific_shop_URL = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\", \"DM\")\n",
    "products_of_shop = requests.get(items_of_specific_shop_URL).json()\n",
    "pprint.pprint(products_of_shop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming records\n",
    "\n",
    "# Use the convenience function to query the API\n",
    "tesco_items = retrieve_products(\"Tesco\")\n",
    "\n",
    "singer.write_schema(stream_name=\"products\", schema=schema,\n",
    "                    key_properties=[])\n",
    "\n",
    "# Write a single record to the stream, that adheres to the schema\n",
    "singer.write_record(stream_name=\"products\", \n",
    "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
    "\n",
    "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
    "    # Write all of the records that you retrieve from the API\n",
    "    singer.write_records(\n",
    "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
    "      records=({**item, \"store_name\": shop}\n",
    "               for item in retrieve_products(shop))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain taps and targets\n",
    "\n",
    "tap-marketing-api | target-csv --config ingest/data_lake.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data transformation pipeline with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV file\n",
    "\n",
    "# Read a csv file and set the headers\n",
    "df = (spark.read\n",
    "      .options(header=True)\n",
    "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a schema\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "pprint(better_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing invalid rows\n",
    "\n",
    "# Specify the option to drop invalid rows\n",
    "ratings = (spark\n",
    "           .read\n",
    "           .options(header=True, mode=\"DROPMALFORMED\")\n",
    "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
    "ratings.show()\n",
    "\n",
    "\"\"\"\n",
    "If you’re interested, try running the code again with mode=\"PERMISSIVE\", which is the default mode. \n",
    "By the way, the mode is a case insensitive parameter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling unknown data\n",
    "\n",
    "print(\"BEFORE\")\n",
    "ratings.show()\n",
    "\n",
    "print(\"AFTER\")\n",
    "# Replace nulls with arbitrary value on column subset\n",
    "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditionally replacing values\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting and renaming columns\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the columns and rename the \"absorption_rate\" column\n",
    "result = ratings.select([col(\"brand\"),\n",
    "                         col(\"model\"),\n",
    "                         col(\"absorption_rate\").alias(\"absorbency\")])\n",
    "\n",
    "# Show only unique values\n",
    "result.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and aggregating data\n",
    "\n",
    "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
    "\n",
    "aggregated = (purchased\n",
    "              # Group rows by 'Country'\n",
    "              .groupBy(col('Country'))\n",
    "              .agg(\n",
    "                # Calculate the average salary per group\n",
    "                avg('Salary').alias('average_salary'),\n",
    "                # Calculate the standard deviation per group and rename\n",
    "                stddev_samp('Salary'),\n",
    "                # Retain the highest salary per group and rename\n",
    "                sfmax('Salary').alias('highest_salary')\n",
    "              )\n",
    "             )\n",
    "\n",
    "aggregated.show()\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    +-------+--------------+-------------------+--------------+\n",
    "    |Country|average_salary|stddev_samp(Salary)|highest_salary|\n",
    "    +-------+--------------+-------------------+--------------+\n",
    "    |Germany|       63000.0|                NaN|         63000|\n",
    "    | France|       48000.0|                NaN|         48000|\n",
    "    |  Spain|       62000.0| 12727.922061357855|         71000|\n",
    "    +-------+--------------+-------------------+--------------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a deployable artifact\n",
    "\n",
    "zip --recurse-paths pydiaper.zip . -i spark_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submitting your Spark job\n",
    "\n",
    "spark-submit --py-files spark_pipelines/pydiaper/pydiaper.zip spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your data pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating in-memory DataFrames\n",
    "\n",
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
    "\n",
    "# Create a tuple of records\n",
    "data = (\n",
    "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
    "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
    "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
    ")\n",
    "\n",
    "# Create a DataFrame from these records\n",
    "frame = spark.createDataFrame(data)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing and orchestrating a workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the DAG schedule\n",
    "\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "\n",
    "reporting_dag = DAG(\n",
    "    dag_id=\"publish_EMEA_sales_report\", \n",
    "    # Insert the cron expression\n",
    "    schedule_interval=\"0 7 * * 1\",\n",
    "    start_date=datetime(2019, 11, 24),\n",
    "    default_args={\"owner\": \"sales\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying operator dependencies\n",
    "# https://assets.datacamp.com/production/repositories/4724/datasets/12aa3b0063022c0bd568493b3b2653f450796ee9/Example_pizza.png\n",
    "\n",
    "# Specify direction using verbose method\n",
    "prepare_crust.set_downstream(apply_tomato_sauce)\n",
    "\n",
    "tasks_with_tomato_sauce_parent = [add_cheese, add_ham, add_olives, add_mushroom]\n",
    "for task in tasks_with_tomato_sauce_parent:\n",
    "    # Specify direction using verbose method on relevant task\n",
    "    apply_tomato_sauce.set_downstream(task)\n",
    "\n",
    "# Specify direction using bitshift operator\n",
    "tasks_with_tomato_sauce_parent >> bake_pizza\n",
    "\n",
    "# Specify direction using verbose method\n",
    "bake_pizza.set_upstream(prepare_oven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a DAG for daily pipelines\n",
    "\n",
    "# Create a DAG object\n",
    "dag = DAG(\n",
    "  dag_id='optimize_diaper_purchases',\n",
    "  default_args={\n",
    "    # Don't email on failure\n",
    "    'email_on_failure': False,\n",
    "    # Specify when tasks should have started earliest\n",
    "    'start_date': datetime(2019, 6, 25)\n",
    "  },\n",
    "  # Run the DAG daily\n",
    "  schedule_interval='@daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling bash scripts with Airflow\n",
    "\n",
    "config = os.path.join(os.environ[\"AIRFLOW_HOME\"], \n",
    "                      \"scripts\",\n",
    "                      \"configs\", \n",
    "                      \"data_lake.conf\")\n",
    "\n",
    "ingest = BashOperator(\n",
    "  # Assign a descriptive id\n",
    "  task_id=\"ingest_data\", \n",
    "  # Complete the ingestion pipeline\n",
    "  bash_command='tap-marketing-api | target-csv --config %s' % config,\n",
    "  dag=dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling Spark jobs with Airflow\n",
    "\n",
    "# Import the operator\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "\n",
    "# Set the path for our files.\n",
    "entry_point = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"scripts\", \"clean_ratings.py\")\n",
    "dependency_path = os.path.join(os.environ[\"AIRFLOW_HOME\"], \"dependencies\", \"pydiaper.zip\")\n",
    "\n",
    "with DAG('data_pipeline', start_date=datetime(2019, 6, 25),\n",
    "         schedule_interval='@daily') as dag:\n",
    "  \t# Define task clean, running a cleaning job.\n",
    "    clean_data = SparkSubmitOperator(\n",
    "        application=entry_point, \n",
    "        py_files=dependency_path,\n",
    "        task_id='clean_data',\n",
    "        conn_id='spark_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling the full data pipeline with Airflow\n",
    "\n",
    "spark_args = {\"py_files\": dependency_path,\n",
    "              \"conn_id\": \"spark_default\"}\n",
    "# Define ingest, clean and transform job.\n",
    "with dag:\n",
    "    ingest = BashOperator(task_id='Ingest_data', bash_command='tap-marketing-api | target-csv --config %s' % config)\n",
    "    clean = SparkSubmitOperator(application=clean_path, task_id='clean_data', **spark_args)\n",
    "    insight = SparkSubmitOperator(application=transform_path, task_id='show_report', **spark_args)\n",
    "    \n",
    "    # set triggering sequence\n",
    "    ingest >> clean >> insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering from deployed but broken DAGs\n",
    "\n",
    "\"\"\"\n",
    "An Apache Airflow DAG used in course material.\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    #    \"owner\": \"squad-a\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2019, 7, 5),\n",
    "    \"email\": [\"foo@bar.com\"],\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    \"cleaning\",\n",
    "    default_args=default_args,\n",
    "    user_defined_macros={\"env\": Variable.get(\"environment\")},\n",
    "    schedule_interval=\"0 5 */2 * *\"\n",
    ")\n",
    "\n",
    "\n",
    "def say(what):\n",
    "    print(what)\n",
    "\n",
    "\n",
    "with dag:\n",
    "    say_hello = BashOperator(task_id=\"say-hello\", bash_command=\"echo Hello,\")\n",
    "    say_world = BashOperator(task_id=\"say-world\", bash_command=\"echo World\")\n",
    "    shout = PythonOperator(task_id=\"shout\",\n",
    "                           python_callable=say,\n",
    "                           op_kwargs={'what': '!'})\n",
    "\n",
    "    say_hello >> say_world >> shout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running tests on Airflow\n",
    "\n",
    "\"\"\"\n",
    "An Apache Airflow DAG used in course material.\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"squad-a\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2019, 7, 5),\n",
    "    \"email\": [\"foo@bar.com\"],\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    \"cleaning\",\n",
    "    default_args=default_args,\n",
    "    user_defined_macros={\"env\": Variable.get(\"environment\")},\n",
    "    schedule_interval=\"0 5 */2 * *\"\n",
    ")\n",
    "\n",
    "\n",
    "def say(what):\n",
    "    print(what)\n",
    "\n",
    "\n",
    "with dag:\n",
    "    say_hello = BashOperator(task_id=\"say-hello\", bash_command=\"echo Hello,\")\n",
    "    say_world = BashOperator(task_id=\"say-world\", bash_command=\"echo World\")\n",
    "    shout = PythonOperator(task_id=\"shout\",\n",
    "                           python_callable=say,\n",
    "                           op_kwargs={'what': '!'})\n",
    "\n",
    "    say_hello >> say_world >> shout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
