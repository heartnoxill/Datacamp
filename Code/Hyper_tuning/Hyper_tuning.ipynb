{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a Logistic Regression parameter\n",
    "\n",
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = list(X_train.columns)\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a Random Forest parameter\n",
    "\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Random Forest Hyperparameters\n",
    "\n",
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print(\"Confusion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}\".format(\n",
    "  confusion_matrix(y_test, rf_old_predictions),\n",
    "  accuracy_score(y_test, rf_old_predictions))) \n",
    "\n",
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model (using new predictions!)\n",
    "print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, rf_new_predictions))\n",
    "print(\"Accuracy Score: \\n\\n\", accuracy_score(y_test, rf_new_predictions))\n",
    "\n",
    "\"\"\"\n",
    " Confusion Matrix: \n",
    "    \n",
    "     [[276  37]\n",
    "     [ 64  23]] \n",
    "     Accuracy Score: \n",
    "    \n",
    "     0.7475\n",
    "    Confusion Matrix: \n",
    "    \n",
    "     [[300  13]\n",
    "     [ 63  24]]\n",
    "    Accuracy Score: \n",
    "    \n",
    "     0.81\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of KNN\n",
    "\n",
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbours was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))\n",
    "\n",
    "\"\"\"\n",
    "The accuracy of 5, 10, 20 neighbours was 0.7125, 0.765, 0.7825\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating Hyperparameter Choice\n",
    "\n",
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=lr)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([lr, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Learning Curves\n",
    "\n",
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Grid Search functions\n",
    "\n",
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "\n",
    "\t# Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively tune multiple hyperparameters\n",
    "\n",
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2, 4, 6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate,max_depth))\n",
    "\n",
    "# Print the results\n",
    "print(results_list)   \n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "\n",
    "\t# Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])       \n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list = [0.4, 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "    \n",
    "    \t# Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "        \t\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "            \n",
    "# Print results\n",
    "print(results_list)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV with Scikit Learn\n",
    "\n",
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)\n",
    "\n",
    "\"\"\"\n",
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "           estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "                max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                min_samples_leaf=1, min_samples_split=2,\n",
    "                min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
    "                oob_score=False, random_state=None, verbose=0,\n",
    "                warm_start=False),\n",
    "           fit_params=None, iid='warn', n_jobs=4,\n",
    "           param_grid={'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']},\n",
    "           pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "           scoring='roc_auc', verbose=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the best outputs\n",
    "Which of the following parameters must be set in order to be able to directly use the best_estimator_ property for predictions?\n",
    "\n",
    "--> refit = True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the grid search results\n",
    "\n",
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Extract and print the column with a dictionary of hyperparameters used\n",
    "column = cv_results_df.loc[:, [\"params\"]]\n",
    "print(column)\n",
    "\n",
    "# Extract and print the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df['rank_test_score'] == 1 ]\n",
    "print(best_row)\n",
    "\n",
    "\"\"\"\n",
    " mean_fit_time  std_fit_time  mean_score_time  std_score_time param_max_depth param_min_samples_leaf param_n_estimators                                             params  split0_test_score       ...         std_test_score  rank_test_score  split0_train_score  split1_train_score  split2_train_score  split3_train_score  split4_train_score  mean_train_score  std_train_score\n",
    "    0        0.324582      0.004639         0.020115        0.003147              10                      1                100  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...           0.728204       ...               0.029951                9            0.994760            0.995460            0.995433            0.998330            0.996505          0.996098     1.248173e-03\n",
    "    1        0.671539      0.018667         0.039659        0.006038              10                      1                200  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...           0.735397       ...               0.027214                4            0.996087            0.995598            0.996509            0.999351            0.997369          0.996983     1.319340e-03\n",
    "    2        0.977654      0.011564         0.054303        0.003766              10                      1                300  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...           0.729294       ...               0.027756                1            0.996046            0.996136            0.996882            0.999655            0.997122          0.997168     1.311001e-03\n",
    "    3        0.313844      0.008520         0.017680        0.002485              10                      2                100  {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...           0.728204       ...               0.027946                3            0.988662            0.984090            0.991403            0.994205            0.990968          0.989866     3.382634e-03\n",
    "    4        0.645001      0.003995         0.034633        0.004523              10                      2                200  {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...           0.728858       ...               0.032881                2            0.988980            0.990355            0.993321            0.994922            0.992832          0.992082     2.134008e-03\n",
    "    5        0.967691      0.010192         0.057124        0.003817              10                      2                300  {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...           0.721665       ...               0.033275                6            0.988717            0.990755            0.992756            0.995088            0.992531          0.991969     2.131063e-03\n",
    "    6        0.342355      0.002576         0.020001        0.002485              20                      1                100  {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...           0.720684       ...               0.022226               11            1.000000            1.000000            1.000000            1.000000            1.000000          1.000000     4.965068e-17\n",
    "    7        0.724901      0.003929         0.034221        0.002700              20                      1                200  {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...           0.725262       ...               0.030262               10            1.000000            1.000000            1.000000            1.000000            1.000000          1.000000     4.965068e-17\n",
    "    8        1.078176      0.013817         0.056005        0.002604              20                      1                300  {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...           0.732672       ...               0.024880                5            1.000000            1.000000            1.000000            1.000000            1.000000          1.000000     0.000000e+00\n",
    "    9        0.351497      0.010176         0.020151        0.004323              20                      2                100  {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...           0.718178       ...               0.025444               12            0.997567            0.999379            0.998289            0.999724            0.998383          0.998668     7.821910e-04\n",
    "    10       0.695131      0.018459         0.040476        0.001271              20                      2                200  {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...           0.726896       ...               0.021853                8            0.998728            0.999089            0.998579            0.999710            0.998904          0.999002     3.932301e-04\n",
    "    11       0.792999      0.021011         0.030246        0.003575              20                      2                300  {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...           0.730384       ...               0.020686                7            0.997539            0.998979            0.998938            0.999862            0.998904          0.998844     7.443455e-04\n",
    "    \n",
    "    [12 rows x 23 columns]\n",
    "                                                   params\n",
    "    0   {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...\n",
    "    1   {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...\n",
    "    2   {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...\n",
    "    3   {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...\n",
    "    4   {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...\n",
    "    5   {'max_depth': 10, 'min_samples_leaf': 2, 'n_es...\n",
    "    6   {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...\n",
    "    7   {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...\n",
    "    8   {'max_depth': 20, 'min_samples_leaf': 1, 'n_es...\n",
    "    9   {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...\n",
    "    10  {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...\n",
    "    11  {'max_depth': 20, 'min_samples_leaf': 2, 'n_es...\n",
    "       mean_fit_time  std_fit_time  mean_score_time  std_score_time param_max_depth param_min_samples_leaf param_n_estimators                                             params  split0_test_score       ...         std_test_score  rank_test_score  split0_train_score  split1_train_score  split2_train_score  split3_train_score  split4_train_score  mean_train_score  std_train_score\n",
    "    2       0.977654      0.011564         0.054303        0.003766              10                      1                300  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...           0.729294       ...               0.027756                1            0.996046            0.996136            0.996882            0.999655            0.997122          0.997168         0.001311\n",
    "    \n",
    "    [1 rows x 23 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the best results\n",
    "\n",
    "# Print out the ROC_AUC score from the best-performing square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Create a variable from the row related to the best-performing square\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the n_estimators parameter from the best-performing square and print\n",
    "best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]\n",
    "print(best_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best results\n",
    "\n",
    "# See what type of object the best_estimator_ property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Now create a confusion matrix \n",
    "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Sample Hyperparameters\n",
    "\n",
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = list(np.linspace(0.01,1.5,200))\n",
    "\n",
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search.\n",
    "random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Search with Random Forest\n",
    "\n",
    "# Create lists for criterion and max_features\n",
    "criterion_list = ['gini', 'entropy']\n",
    "max_feature_list = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "# Create a list of values for the max_depth hyperparameter\n",
    "max_depth_list = list(range(3,56))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search\n",
    "combinations_random_chosen = random.sample(combinations_list, 150)\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a Random Search\n",
    "\n",
    "def sample_and_visualize_hyperparameters(n_samples):\n",
    "\n",
    "  # If asking for all combinations, just return the entire list.\n",
    "  if n_samples == len(combinations_list):\n",
    "    combinations_random_chosen = combinations_list\n",
    "  else:\n",
    "    combinations_random_chosen = []\n",
    "    random_combinations_index = np.random.choice(range(0, len(combinations_list)), n_samples, replace=False)\n",
    "    combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "    \n",
    "  # Pull out the X and Y to plot\n",
    "  rand_y, rand_x = [x[0] for x in combinations_random_chosen], [x[1] for x in combinations_random_chosen]\n",
    "\n",
    "  # Plot \n",
    "  plt.clf() \n",
    "  plt.scatter(rand_y, rand_x, c=['blue']*len(combinations_random_chosen))\n",
    "  plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "  plt.gca().set_xlim(x_lims)\n",
    "  plt.gca().set_ylim(y_lims)\n",
    "  plt.show()\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# Confirm how many hyperparameter combinations & print\n",
    "number_combs = len(combinations_list)\n",
    "print(number_combs)\n",
    "\n",
    "# Sample and visualise specified combinations\n",
    "for x in [50, 500, 1500]:\n",
    "    sample_and_visualize_hyperparameters(x)\n",
    "    \n",
    "# Sample all the hyperparameter combinations & visualise\n",
    "sample_and_visualize_hyperparameters(number_combs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RandomizedSearchCV Object\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'learning_rate': np.linspace(0.1,2,150), 'min_samples_leaf': list(range(20,65))} \n",
    "\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_GBM_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "print(random_GBM_class.cv_results_['param_min_samples_leaf'])\n",
    "\n",
    "\"\"\"\n",
    "[1.1073825503355705 1.0691275167785235 0.4697986577181208\n",
    "     1.2476510067114095 1.5664429530201343 1.7577181208053692\n",
    "     1.859731543624161 1.5791946308724834 0.5463087248322147\n",
    "     1.7577181208053692]\n",
    "    [47 54 61 30 63 32 60 43 38 27]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearchCV in Scikit Learn\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
    "\n",
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True )\n",
    "\n",
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_rf_class.cv_results_['param_max_depth'])\n",
    "print(random_rf_class.cv_results_['param_max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid and Random Search Side by Side\n",
    "\n",
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]\n",
    "\n",
    "# Print result\n",
    "print(grid_combinations_chosen)\n",
    "\n",
    "\"\"\"\n",
    "[[0.01, 5], [0.01, 6], [0.01, 7], [0.01, 8], [0.01, 9], [0.01, 10], [0.01, 11], [0.01, 12], [0.01, 13], [0.01, 14], [0.01, 15], [0.01, 16], [0.01, 17], [0.01, 18], [0.01, 19], [0.01, 20], [0.01, 21], [0.01, 22], [0.01, 23], [0.01, 24], [0.025025125628140705, 5], [0.025025125628140705, 6], [0.025025125628140705, 7], [0.025025125628140705, 8], [0.025025125628140705, 9], [0.025025125628140705, 10], [0.025025125628140705, 11], [0.025025125628140705, 12], [0.025025125628140705, 13], [0.025025125628140705, 14], [0.025025125628140705, 15], [0.025025125628140705, 16], [0.025025125628140705, 17], [0.025025125628140705, 18], [0.025025125628140705, 19], [0.025025125628140705, 20], [0.025025125628140705, 21], [0.025025125628140705, 22], [0.025025125628140705, 23], [0.025025125628140705, 24], [0.04005025125628141, 5], [0.04005025125628141, 6], [0.04005025125628141, 7], [0.04005025125628141, 8], [0.04005025125628141, 9], [0.04005025125628141, 10], [0.04005025125628141, 11], [0.04005025125628141, 12], [0.04005025125628141, 13], [0.04005025125628141, 14], [0.04005025125628141, 15], [0.04005025125628141, 16], [0.04005025125628141, 17], [0.04005025125628141, 18], [0.04005025125628141, 19], [0.04005025125628141, 20], [0.04005025125628141, 21], [0.04005025125628141, 22], [0.04005025125628141, 23], [0.04005025125628141, 24], [0.055075376884422114, 5], [0.055075376884422114, 6], [0.055075376884422114, 7], [0.055075376884422114, 8], [0.055075376884422114, 9], [0.055075376884422114, 10], [0.055075376884422114, 11], [0.055075376884422114, 12], [0.055075376884422114, 13], [0.055075376884422114, 14], [0.055075376884422114, 15], [0.055075376884422114, 16], [0.055075376884422114, 17], [0.055075376884422114, 18], [0.055075376884422114, 19], [0.055075376884422114, 20], [0.055075376884422114, 21], [0.055075376884422114, 22], [0.055075376884422114, 23], [0.055075376884422114, 24], [0.07010050251256282, 5], [0.07010050251256282, 6], [0.07010050251256282, 7], [0.07010050251256282, 8], [0.07010050251256282, 9], [0.07010050251256282, 10], [0.07010050251256282, 11], [0.07010050251256282, 12], [0.07010050251256282, 13], [0.07010050251256282, 14], [0.07010050251256282, 15], [0.07010050251256282, 16], [0.07010050251256282, 17], [0.07010050251256282, 18], [0.07010050251256282, 19], [0.07010050251256282, 20], [0.07010050251256282, 21], [0.07010050251256282, 22], [0.07010050251256282, 23], [0.07010050251256282, 24], [0.08512562814070351, 5], [0.08512562814070351, 6], [0.08512562814070351, 7], [0.08512562814070351, 8], [0.08512562814070351, 9], [0.08512562814070351, 10], [0.08512562814070351, 11], [0.08512562814070351, 12], [0.08512562814070351, 13], [0.08512562814070351, 14], [0.08512562814070351, 15], [0.08512562814070351, 16], [0.08512562814070351, 17], [0.08512562814070351, 18], [0.08512562814070351, 19], [0.08512562814070351, 20], [0.08512562814070351, 21], [0.08512562814070351, 22], [0.08512562814070351, 23], [0.08512562814070351, 24], [0.10015075376884422, 5], [0.10015075376884422, 6], [0.10015075376884422, 7], [0.10015075376884422, 8], [0.10015075376884422, 9], [0.10015075376884422, 10], [0.10015075376884422, 11], [0.10015075376884422, 12], [0.10015075376884422, 13], [0.10015075376884422, 14], [0.10015075376884422, 15], [0.10015075376884422, 16], [0.10015075376884422, 17], [0.10015075376884422, 18], [0.10015075376884422, 19], [0.10015075376884422, 20], [0.10015075376884422, 21], [0.10015075376884422, 22], [0.10015075376884422, 23], [0.10015075376884422, 24], [0.11517587939698493, 5], [0.11517587939698493, 6], [0.11517587939698493, 7], [0.11517587939698493, 8], [0.11517587939698493, 9], [0.11517587939698493, 10], [0.11517587939698493, 11], [0.11517587939698493, 12], [0.11517587939698493, 13], [0.11517587939698493, 14], [0.11517587939698493, 15], [0.11517587939698493, 16], [0.11517587939698493, 17], [0.11517587939698493, 18], [0.11517587939698493, 19], [0.11517587939698493, 20], [0.11517587939698493, 21], [0.11517587939698493, 22], [0.11517587939698493, 23], [0.11517587939698493, 24], [0.13020100502512563, 5], [0.13020100502512563, 6], [0.13020100502512563, 7], [0.13020100502512563, 8], [0.13020100502512563, 9], [0.13020100502512563, 10], [0.13020100502512563, 11], [0.13020100502512563, 12], [0.13020100502512563, 13], [0.13020100502512563, 14], [0.13020100502512563, 15], [0.13020100502512563, 16], [0.13020100502512563, 17], [0.13020100502512563, 18], [0.13020100502512563, 19], [0.13020100502512563, 20], [0.13020100502512563, 21], [0.13020100502512563, 22], [0.13020100502512563, 23], [0.13020100502512563, 24], [0.14522613065326634, 5], [0.14522613065326634, 6], [0.14522613065326634, 7], [0.14522613065326634, 8], [0.14522613065326634, 9], [0.14522613065326634, 10], [0.14522613065326634, 11], [0.14522613065326634, 12], [0.14522613065326634, 13], [0.14522613065326634, 14], [0.14522613065326634, 15], [0.14522613065326634, 16], [0.14522613065326634, 17], [0.14522613065326634, 18], [0.14522613065326634, 19], [0.14522613065326634, 20], [0.14522613065326634, 21], [0.14522613065326634, 22], [0.14522613065326634, 23], [0.14522613065326634, 24], [0.16025125628140705, 5], [0.16025125628140705, 6], [0.16025125628140705, 7], [0.16025125628140705, 8], [0.16025125628140705, 9], [0.16025125628140705, 10], [0.16025125628140705, 11], [0.16025125628140705, 12], [0.16025125628140705, 13], [0.16025125628140705, 14], [0.16025125628140705, 15], [0.16025125628140705, 16], [0.16025125628140705, 17], [0.16025125628140705, 18], [0.16025125628140705, 19], [0.16025125628140705, 20], [0.16025125628140705, 21], [0.16025125628140705, 22], [0.16025125628140705, 23], [0.16025125628140705, 24], [0.17527638190954775, 5], [0.17527638190954775, 6], [0.17527638190954775, 7], [0.17527638190954775, 8], [0.17527638190954775, 9], [0.17527638190954775, 10], [0.17527638190954775, 11], [0.17527638190954775, 12], [0.17527638190954775, 13], [0.17527638190954775, 14], [0.17527638190954775, 15], [0.17527638190954775, 16], [0.17527638190954775, 17], [0.17527638190954775, 18], [0.17527638190954775, 19], [0.17527638190954775, 20], [0.17527638190954775, 21], [0.17527638190954775, 22], [0.17527638190954775, 23], [0.17527638190954775, 24], [0.19030150753768846, 5], [0.19030150753768846, 6], [0.19030150753768846, 7], [0.19030150753768846, 8], [0.19030150753768846, 9], [0.19030150753768846, 10], [0.19030150753768846, 11], [0.19030150753768846, 12], [0.19030150753768846, 13], [0.19030150753768846, 14], [0.19030150753768846, 15], [0.19030150753768846, 16], [0.19030150753768846, 17], [0.19030150753768846, 18], [0.19030150753768846, 19], [0.19030150753768846, 20], [0.19030150753768846, 21], [0.19030150753768846, 22], [0.19030150753768846, 23], [0.19030150753768846, 24], [0.20532663316582916, 5], [0.20532663316582916, 6], [0.20532663316582916, 7], [0.20532663316582916, 8], [0.20532663316582916, 9], [0.20532663316582916, 10], [0.20532663316582916, 11], [0.20532663316582916, 12], [0.20532663316582916, 13], [0.20532663316582916, 14], [0.20532663316582916, 15], [0.20532663316582916, 16], [0.20532663316582916, 17], [0.20532663316582916, 18], [0.20532663316582916, 19], [0.20532663316582916, 20], [0.20532663316582916, 21], [0.20532663316582916, 22], [0.20532663316582916, 23], [0.20532663316582916, 24], [0.22035175879396987, 5], [0.22035175879396987, 6], [0.22035175879396987, 7], [0.22035175879396987, 8], [0.22035175879396987, 9], [0.22035175879396987, 10], [0.22035175879396987, 11], [0.22035175879396987, 12], [0.22035175879396987, 13], [0.22035175879396987, 14], [0.22035175879396987, 15], [0.22035175879396987, 16], [0.22035175879396987, 17], [0.22035175879396987, 18], [0.22035175879396987, 19], [0.22035175879396987, 20], [0.22035175879396987, 21], [0.22035175879396987, 22], [0.22035175879396987, 23], [0.22035175879396987, 24]]\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]\n",
    "\n",
    "# Create a list of sample indexes\n",
    "sample_indexes = list(range(0,len(combinations_list)))\n",
    "\n",
    "# Randomly sample 300 indexes\n",
    "random_indexes = np.random.choice(sample_indexes, 300, replace=False)\n",
    "\n",
    "# Use indexes to create random sample\n",
    "random_combinations_chosen = [combinations_list[index] for index in random_indexes]\n",
    "\n",
    "# Call the function to produce the visualization\n",
    "visualize_search(grid_combinations_chosen, random_combinations_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Coarse to Fine\n",
    "\n",
    "def visualize_hyperparameter(name):\n",
    "    plt.clf()\n",
    "    plt.scatter(results_df[name],results_df['accuracy'], c=['blue']*500)\n",
    "    plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\n",
    "    plt.gca().set_ylim([0,100])\n",
    "    \n",
    "# Confirm the size of the combinations_list\n",
    "print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "visualize_hyperparameter('max_depth')\n",
    "visualize_hyperparameter('min_samples_leaf')\n",
    "visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarse to Fine Iterations\n",
    "\n",
    "def visualize_second():\n",
    "    for name in results_df2.columns[0:2]:\n",
    "        plt.clf()\n",
    "        plt.scatter(results_df[name],results_df['accuracy'], c=['blue']*500)\n",
    "        plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\n",
    "        plt.gca().set_ylim([0,100])\n",
    "        x_line = 20\n",
    "        if name == \"learn_rate\":\n",
    "            x_line = 1\n",
    "        plt.axvline(x=x_line, color=\"red\", linewidth=4)\n",
    "\n",
    "        \n",
    "############################################################################################################\n",
    "\n",
    "# Use the provided function to visualize the first results\n",
    "visualize_first()\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine:\n",
    "max_depth_list = list(range(1,21))\n",
    "learn_rate_list = np.linspace(0.001,1,50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "visualize_second()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes Rule in Python\n",
    "\n",
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07\n",
    "\n",
    "# Probability unhappy person will close\n",
    "p_close_unhappy = (p_unhappy_close * p_close) / p_unhappy\n",
    "print(p_close_unhappy)\n",
    "\n",
    "\"\"\"0.16333333333333336\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Hyperparameter tuning with Hyperopt\n",
    "\n",
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "print(best)\n",
    "\n",
    "\"\"\"\n",
    "0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]\n",
    "  5%|5         | 1/20 [00:00<00:03,  5.75it/s, best loss: 0.26759418985474637]\n",
    " 10%|#         | 2/20 [00:00<00:03,  5.83it/s, best loss: 0.2549063726593165] \n",
    " 15%|#5        | 3/20 [00:00<00:02,  6.18it/s, best loss: 0.2549063726593165]\n",
    " 20%|##        | 4/20 [00:00<00:02,  6.04it/s, best loss: 0.2549063726593165]\n",
    " 25%|##5       | 5/20 [00:01<00:03,  4.13it/s, best loss: 0.2549063726593165]\n",
    " 30%|###       | 6/20 [00:01<00:03,  4.40it/s, best loss: 0.2549063726593165]\n",
    " 35%|###5      | 7/20 [00:01<00:02,  4.97it/s, best loss: 0.2549063726593165]\n",
    " 40%|####      | 8/20 [00:01<00:02,  5.30it/s, best loss: 0.2549063726593165]\n",
    " 45%|####5     | 9/20 [00:01<00:01,  5.65it/s, best loss: 0.2549063726593165]\n",
    " 50%|#####     | 10/20 [00:01<00:01,  5.88it/s, best loss: 0.2549063726593165]\n",
    " 55%|#####5    | 11/20 [00:02<00:01,  6.17it/s, best loss: 0.2549063726593165]\n",
    " 60%|######    | 12/20 [00:02<00:01,  5.33it/s, best loss: 0.2549063726593165]\n",
    " 65%|######5   | 13/20 [00:02<00:01,  4.99it/s, best loss: 0.2549063726593165]\n",
    " 70%|#######   | 14/20 [00:03<00:01,  3.19it/s, best loss: 0.2525688142203555]\n",
    " 75%|#######5  | 15/20 [00:03<00:01,  3.48it/s, best loss: 0.2525688142203555]\n",
    " 80%|########  | 16/20 [00:03<00:01,  3.82it/s, best loss: 0.2525688142203555]\n",
    " 85%|########5 | 17/20 [00:04<00:01,  2.94it/s, best loss: 0.24246856171404285]\n",
    " 90%|######### | 18/20 [00:04<00:00,  3.38it/s, best loss: 0.24246856171404285]\n",
    " 95%|#########5| 19/20 [00:04<00:00,  3.65it/s, best loss: 0.24246856171404285]\n",
    "100%|##########| 20/20 [00:04<00:00,  4.13it/s, best loss: 0.24246856171404285]\n",
    "100%|##########| 20/20 [00:04<00:00,  4.36it/s, best loss: 0.24246856171404285]\n",
    "    {'learning_rate': 0.11310589268581149, 'max_depth': 6.0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Hyperparameter Tuning with TPOT\n",
    "\n",
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing TPOT's stability\n",
    "\n",
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))\n",
    "\n",
    "\"\"\"\n",
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "Generation 1 - Current best internal CV score: 0.7549688742218555\n",
    "Generation 2 - Current best internal CV score: 0.7549688742218555\n",
    "\n",
    "Best pipeline: DecisionTreeClassifier(input_matrix, criterion=gini, max_depth=7, min_samples_leaf=11, min_samples_split=12)\n",
    "0.75\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=122)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))\n",
    "\n",
    "\"\"\"\n",
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "Generation 1 - Current best internal CV score: 0.7675066876671917\n",
    "Generation 2 - Current best internal CV score: 0.7675066876671917\n",
    "\n",
    "Best pipeline: KNeighborsClassifier(MaxAbsScaler(input_matrix), n_neighbors=57, p=1, weights=distance)\n",
    "0.75\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=99)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))\n",
    "\n",
    "\"\"\"\n",
    "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
    "Generation 1 - Current best internal CV score: 0.8075326883172079\n",
    "Generation 2 - Current best internal CV score: 0.8075326883172079\n",
    "\n",
    "Best pipeline: RandomForestClassifier(SelectFwe(input_matrix, alpha=0.033), bootstrap=False, criterion=gini, max_features=1.0, min_samples_leaf=19, min_samples_split=10, n_estimators=100)\n",
    "0.78\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "You can see that TPOT is quite unstable when only running with low generations, population size and offspring. \n",
    "The first model chosen was a Decision Tree, then a K-nearest Neighbor model and finally a Random Forest. \n",
    "Increasing the generations, population size and offspring and running this for a long time will assist to \n",
    "produce better models and more stable results. Don't hesitate to try it yourself on your own machine!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
