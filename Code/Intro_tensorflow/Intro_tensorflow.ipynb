{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data as constants\n",
    "\n",
    "# Import constant from TensorFlow\n",
    "from tensorflow import constant\n",
    "\n",
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('\\n The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('\\n The shape is:', credit_constant.shape)\n",
    "\n",
    "\"\"\"\n",
    "The datatype is: <dtype: 'float64'>\n",
    "The shape is: (30000, 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables\n",
    "\n",
    "# Define the 1-dimensional variable A1\n",
    "A1 = Variable([1, 2, 3, 4]) # tf.Variable\n",
    "\n",
    "# Print the variable A1\n",
    "print('\\n A1: ', A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print('\\n B1: ', B1)\n",
    "\n",
    "\"\"\"\n",
    "A1:  <tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n",
    "    \n",
    "B1:  [1 2 3 4]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing element-wise multiplication\n",
    "\n",
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = multiply(A1, B1)\n",
    "C23 = multiply(A23, B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('\\n C1: {}'.format(C1.numpy()))\n",
    "print('\\n C23: {}'.format(C23.numpy()))\n",
    "\n",
    "\"\"\"\n",
    "C1: [1 2 3 4]\n",
    "    \n",
    "     C23: [[1 2 3]\n",
    "     [1 6 4]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with matrix multiplication\n",
    "\n",
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features, params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill - billpred\n",
    "print(error.numpy())\n",
    "\n",
    "\"\"\"\n",
    "[[-1687]\n",
    "     [-3218]\n",
    "     [-1933]\n",
    "     [57850]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping tensors\n",
    "\n",
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = reshape(gray_tensor, (784, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = reshape(color_tensor, (2352, 1)) # color tensor has 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing with gradients\n",
    "\n",
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = Variable(x0)\n",
    "\twith GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = multiply(x, x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))\n",
    "\n",
    "\"\"\"-2.0\n",
    "    2.0\n",
    "    0.0\n",
    "\n",
    "Notice that the slope is positive at x = 1, which means that we can lower the loss by reducing x. \n",
    "The slope is negative at x = -1, which means that we can lower the loss by increasing x. \n",
    "The slope at x = 0 is 0, which means that we cannot lower the loss by either increasing or decreasing x. \n",
    "This is because the loss is minimized at x = 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with image data\n",
    "\n",
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "output = matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = reduce_sum(output)\n",
    "print(prediction.numpy())\n",
    "\n",
    "\"\"\"\n",
    "Your model found that prediction=1.0 and correctly classified the letter as a K. \n",
    "In the coming chapters, you will use data to train a model, model, and then combine this with matrix multiplication, \n",
    "matmul(letter, model), as we have done here, to make predictions about the classes of objects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using pandas\n",
    "\n",
    "# Import pandas under the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the path to a string variable named data_path\n",
    "data_path = 'kc_house_data.csv'\n",
    "\n",
    "# Load the dataset as a dataframe named housing\n",
    "housing = pd.read_csv(data_path)\n",
    "\n",
    "# Print the price column of housing\n",
    "print(housing['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the data type\n",
    "\n",
    "# Import numpy and tensorflow with their standard aliases\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use a numpy array to define price as a 32-bit float\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Define waterfront as a Boolean using cast\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "\n",
    "# Print price and waterfront\n",
    "print(price)\n",
    "print(waterfront)\n",
    "\n",
    "\"\"\"\n",
    "[221900. 538000. 180000. ... 402101. 400000. 325000.]\n",
    "tf.Tensor([False False False ... False False False], shape=(21613,), dtype=bool)\n",
    "\n",
    "Notice that printing price yielded a numpy array; whereas printing waterfront yielded a tf.Tensor().\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions in TensorFlow\n",
    "\n",
    "# Import the keras module from tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Compute the mean squared error (mse)\n",
    "loss = keras.losses.mse(price, predictions)\n",
    "\n",
    "# Print the mean squared error (mse)\n",
    "print(loss.numpy())\n",
    "\n",
    "\"\"\"141171604777.12717\"\"\"\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# Import the keras module from tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Compute the mean absolute error (mae)\n",
    "loss = keras.losses.mae(price, predictions)\n",
    "\n",
    "# Print the mean absolute error (mae)\n",
    "print(loss.numpy())\n",
    "\n",
    "\"\"\"268827.99302088\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "You may have noticed that the MAE was much smaller than the MSE, even though price and predictions were the same. \n",
    "This is because the different loss functions penalize deviations of predictions from price differently.\n",
    "MSE does not like large deviations and punishes them harshly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the loss function\n",
    "\n",
    "# Initialize a variable named scalar\n",
    "scalar = Variable(1.0, float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "  \treturn scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "\t# Compute the predicted values\n",
    "\tpredictions = model(scalar, features)\n",
    "    \n",
    "\t# Return the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())\n",
    "\n",
    "\"\"\"\n",
    "3.0\n",
    "As you will see in the following lessons, this exercise was the equivalent of evaluating the loss function for \n",
    "a linear regression where the intercept is 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a linear regression\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features = size_log):\n",
    "\treturn intercept + features*slope\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "\treturn keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Compute the loss for different slope and intercept values\n",
    "print(loss_function(0.1, 0.1).numpy())\n",
    "print(loss_function(0.1, 0.5).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear model\n",
    "\n",
    "# Initialize an adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "\n",
    "\t# Print every 10th value of the loss\n",
    "\tif j % 10 == 0:\n",
    "\t\tprint(loss_function(intercept, slope).numpy())\n",
    "\n",
    "# Plot data and regression line\n",
    "plot_results(intercept, slope)\n",
    "\n",
    "\"\"\"\n",
    "9.669482\n",
    "11.726698\n",
    "1.1193314\n",
    "1.6605737\n",
    "0.7982884\n",
    "0.8017316\n",
    "0.6106565\n",
    "0.59997976\n",
    "0.5811015\n",
    "0.5576158\n",
    "\n",
    "Notice that we printed loss_function(intercept, slope) every 10th execution for 100 executions. \n",
    "Each time, the loss got closer to the minimum as the optimizer moved the slope and intercept parameters \n",
    "closer to their optimal values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
    "\treturn params[0] + feature1*params[1] + feature2*params[2]\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(params, feature1, feature2)\n",
    "  \n",
    "\t# Use the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Define the optimize operation\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Perform minimization and print trainable variables\n",
    "for j in range(10):\n",
    "\topt.minimize(lambda: loss_function(params), var_list=[params])\n",
    "\tprint_results(params)\n",
    "    \n",
    "\"\"\"\n",
    "loss: 12.418, intercept: 0.101, slope_1: 0.051, slope_2: 0.021\n",
    "loss: 12.404, intercept: 0.102, slope_1: 0.052, slope_2: 0.022\n",
    "loss: 12.391, intercept: 0.103, slope_1: 0.053, slope_2: 0.023\n",
    "loss: 12.377, intercept: 0.104, slope_1: 0.054, slope_2: 0.024\n",
    "loss: 12.364, intercept: 0.105, slope_1: 0.055, slope_2: 0.025\n",
    "loss: 12.351, intercept: 0.106, slope_1: 0.056, slope_2: 0.026\n",
    "loss: 12.337, intercept: 0.107, slope_1: 0.057, slope_2: 0.027\n",
    "loss: 12.324, intercept: 0.108, slope_1: 0.058, slope_2: 0.028\n",
    "loss: 12.311, intercept: 0.109, slope_1: 0.059, slope_2: 0.029\n",
    "loss: 12.297, intercept: 0.110, slope_1: 0.060, slope_2: 0.030\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing to batch train\n",
    "\n",
    "# Define the intercept and slope\n",
    "intercept = Variable(10, float32)\n",
    "slope = Variable(0.5, float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\t# Define the predicted values\n",
    "\treturn intercept + slope*features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "\t# Define the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    " \t# Define the MSE loss\n",
    "\treturn keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a linear model in batches\n",
    "\n",
    "# Initialize adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())\n",
    "\n",
    "\"\"\"    10.217888 0.7016\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linear algebra of dense layers\n",
    "\n",
    "\"\"\"There are two ways to define a dense layer in tensorflow. The first involves the use of low-level, \n",
    "linear algebraic operations. The second makes use of high-level keras operations. \n",
    "In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "https://assets.datacamp.com/production/repositories/3953/datasets/23d6f91f73eb1363c4fd67c83720ca3c84ce20a1/3_2_1_network2.png\n",
    "\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. \n",
    "The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\"\"\"\n",
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
    "\n",
    "\"\"\"dense1's output shape: (1, 2)\"\"\"\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1, weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')\n",
    "\n",
    "\"\"\"prediction: 0.9525741338729858\n",
    "    \n",
    "     actual: 1\"\"\"\n",
    "\n",
    "\"\"\"Our model produces predicted values in the interval between 0 and 1. For the example we considered, \n",
    "the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, is not meaningful,\n",
    "since we have not yet trained our model's parameters.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The low-level approach with multiple examples\n",
    "\n",
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "\n",
    "\"\"\"\n",
    "shape of borrower_features:  (5, 3)\n",
    "    \n",
    "shape of weights1:  (3, 2)\n",
    "\n",
    "shape of bias1:  (1,)\n",
    "\n",
    "shape of dense1:  (5, 2)\n",
    "     \n",
    "Note that our input data, borrower_features, is 5x3 because it consists of 5 examples for 3 features. \n",
    "The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. \n",
    "Additionally, bias1 is a scalar. Finally, dense1 is 5x2, which means that we can multiply it by the following set of weights,\n",
    "weights2, which we defined to be 2x1 in the previous exercise.   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dense layer operation\n",
    "\n",
    "\"\"\"\n",
    "We've now seen how to define dense layers in tensorflow using linear algebra. In this exercise, \n",
    "we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below,\n",
    "which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "https://assets.datacamp.com/production/repositories/3953/datasets/eb2fda20a023befc69b53ff5bd278c2eee73dac8/10_7_3_1_network.png\n",
    "\"\"\"\n",
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)\n",
    "\n",
    "\"\"\"\n",
    "shape of dense1:  (100, 7)\n",
    "    \n",
    "shape of dense2:  (100, 3)\n",
    "\n",
    "shape of predictions:  (100, 1)\n",
    "\n",
    "Note that each layer has 100 rows because the input data contains 100 examples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification problems\n",
    "\n",
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)\n",
    "\n",
    "\"\"\"\n",
    "[[-1.]\n",
    "[-1.]\n",
    "[-1.]\n",
    "[-1.]\n",
    "[-1.]]\n",
    "\n",
    "If you run the code several times, you'll notice that the errors change each time. \n",
    "This is because you're using an untrained model with randomly initialized parameters. \n",
    "Furthermore, the errors fall on the interval between -1 and 1 because default is a binary variable \n",
    "that takes on values of 0 and 1 and outputs is a probability between 0 and 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass classification problems\n",
    "\n",
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer: softmax=appropriate activation func.\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])\n",
    "\n",
    "\"\"\"\n",
    "[[0.119128   0.21407694 0.21403559 0.16607246 0.14679684 0.13989018]\n",
    "     [0.09451684 0.1801574  0.22960506 0.20694265 0.11374877 0.17502937]\n",
    "     [0.119128   0.21407694 0.21403559 0.16607246 0.14679684 0.13989018]\n",
    "     [0.09906525 0.16128646 0.1742211  0.22180668 0.12069351 0.22292699]\n",
    "     [0.119128   0.21407694 0.21403559 0.16607246 0.14679684 0.13989018]]\n",
    "     \n",
    "Notice that each row of outputs sums to one. This is because a row contains the predicted class \n",
    "probabilities for one example. As with the previous exercise, our predictions are not yet informative, \n",
    "since we are using an untrained model with randomly initialized parameters. \n",
    "This is why the model tends to assign similar probabilities to each class.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dangers of local minima\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "\n",
    "\"\"\"\n",
    "4.3801394 0.42052683\n",
    "\n",
    "Notice that we used the same optimizer and loss function, but two different initial values. \n",
    "When we started at 6.0 with x_1, we found the global minimum at 4.38, marked by the dot on the right. \n",
    "When we started at 0.3, we stopped around 0.42 with x_2, the local minimum marked by a dot on the far left.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding local minima\n",
    "\n",
    "\"\"\"\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in \n",
    "one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima \n",
    "first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. \n",
    "We will again use the loss function from the previous problem, which has been defined and is available \n",
    "for you as loss_function().\n",
    "https://assets.datacamp.com/production/repositories/3953/datasets/42876c85cba5c14941a3fac191eff75b41597112/local_minima_dots_4_10.png\n",
    "\"\"\"\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "\n",
    "\"\"\"\n",
    "    4.3150263 0.4205261\n",
    "Recall that the global minimum is approximately 4.38. Notice that opt_1 built momentum, bringing x_1 closer to\n",
    "the global minimum. To the contrary, opt_2, which had a momentum parameter of 0.0, got stuck in the local minimum on the left.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization in TensorFlow\n",
    "\n",
    "# Define the layer 1 weights\n",
    "w1 = Variable(random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(random.normal([7, 1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model and loss function\n",
    "\n",
    "\"\"\"\n",
    "predictions layer is defined as sigmoid(layer1*w2+b2)\n",
    "\"\"\"\n",
    "\n",
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training neural networks with TensorFlow\n",
    "\n",
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)\n",
    "\n",
    "\"\"\"Output --> Confusion Matrix\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level APIs\n",
    "In the final chapter, you'll use high-level APIs in TensorFlow 2 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sequential model in Keras\n",
    "\n",
    "# Define a Keras sequential model\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 16)                12560     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 8)                 136       \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 4)                 36        \n",
    "=================================================================\n",
    "Total params: 12,732\n",
    "Trainable params: 12,732\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "None\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, \n",
    "loss function, and other useful training parameters in a single line of code. \n",
    "Furthermore, the .summary() method allows us to view the model's architecture.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling a sequential model\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model using an adam optimizer and categorical_crossentropy loss function.\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 16)                12560     \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 16)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 4)                 68        \n",
    "=================================================================\n",
    "Total params: 12,628\n",
    "Trainable params: 12,628\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a multiple input model\n",
    "\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "Model: \"model\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "dense (Dense)                   (None, 12)           9420        input_1[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_2 (Dense)                 (None, 12)           9420        input_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_1 (Dense)                 (None, 4)            52          dense[0][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "dense_3 (Dense)                 (None, 4)            52          dense_2[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "add (Add)                       (None, 4)            0           dense_1[0][0]                    \n",
    "                                                                 dense_3[0][0]                    \n",
    "==================================================================================================\n",
    "Total params: 18,944\n",
    "Trainable params: 18,944\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Notice that the .summary() method yields a new column: connected to. \n",
    "This column tells you how layers connect to each other within the network. \n",
    "We can see that dense_2, for instance, is connected to the input_2 layer. \n",
    "We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Keras\n",
    "\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model with the SGD optimizer and categorical_crossentropy loss.\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)\n",
    "\n",
    "\"\"\"\n",
    "Epoch 1/5\n",
    "    \n",
    " 1/32 [..............................] - ETA: 23s - loss: 2.2646\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "26/32 [=======================>......] - ETA: 0s - loss: 1.5090 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "32/32 [==============================] - 1s 2ms/step - loss: 1.4747\n",
    "    Epoch 2/5\n",
    "    \n",
    " 1/32 [..............................] - ETA: 0s - loss: 1.4094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "27/32 [========================>.....] - ETA: 0s - loss: 1.2044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "32/32 [==============================] - 0s 2ms/step - loss: 1.1934\n",
    "    Epoch 3/5\n",
    "    \n",
    " 1/32 [..............................] - ETA: 0s - loss: 1.0047\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "25/32 [======================>.......] - ETA: 0s - loss: 1.0226\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "32/32 [==============================] - 0s 2ms/step - loss: 1.0210\n",
    "    Epoch 4/5\n",
    "    \n",
    " 1/32 [..............................] - ETA: 0s - loss: 0.9127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "24/32 [=====================>........] - ETA: 0s - loss: 0.9353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "32/32 [==============================] - 0s 2ms/step - loss: 0.9297\n",
    "    Epoch 5/5\n",
    "    \n",
    " 1/32 [..............................] - ETA: 0s - loss: 0.8553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "23/32 [====================>.........] - ETA: 0s - loss: 0.8519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "32/32 [==============================] - 0s 2ms/step - loss: 0.8452\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and validation with Keras\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "Epoch 1/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 28s - loss: 1.4231 - accuracy: 0.3125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "21/29 [====================>.........] - ETA: 0s - loss: 1.3871 - accuracy: 0.3030 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 3s 59ms/step - loss: 1.3545 - accuracy: 0.3324 - val_loss: 1.2242 - val_accuracy: 0.2900\n",
    "    Epoch 2/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 1.3104 - accuracy: 0.2500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "25/29 [========================>.....] - ETA: 0s - loss: 1.0650 - accuracy: 0.6364\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 9ms/step - loss: 1.0533 - accuracy: 0.6447 - val_loss: 1.0445 - val_accuracy: 0.5900\n",
    "    Epoch 3/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.9369 - accuracy: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.8804 - accuracy: 0.7636 - val_loss: 0.8102 - val_accuracy: 0.7000\n",
    "    Epoch 4/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.8435 - accuracy: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.7142 - accuracy: 0.8028 - val_loss: 0.7993 - val_accuracy: 0.6900\n",
    "    Epoch 5/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.7524 - accuracy: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.6524 - accuracy: 0.8449 - val_loss: 0.6350 - val_accuracy: 0.7300\n",
    "    Epoch 6/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.7238 - accuracy: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.8700 - val_loss: 0.7719 - val_accuracy: 0.5900\n",
    "    Epoch 7/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.8442 - accuracy: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "27/29 [==========================>...] - ETA: 0s - loss: 0.5204 - accuracy: 0.8706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 8ms/step - loss: 0.5160 - accuracy: 0.8743 - val_loss: 0.4521 - val_accuracy: 0.9800\n",
    "    Epoch 8/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.4912 - accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.4448 - accuracy: 0.9150 - val_loss: 0.5926 - val_accuracy: 0.7100\n",
    "    Epoch 9/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.4262 - accuracy: 0.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 8ms/step - loss: 0.3948 - accuracy: 0.9326 - val_loss: 0.3892 - val_accuracy: 0.9400\n",
    "    Epoch 10/10\n",
    "    \n",
    " 1/29 [>.............................] - ETA: 0s - loss: 0.3319 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "29/29 [==============================] - 0s 7ms/step - loss: 0.3480 - accuracy: 0.9567 - val_loss: 0.3641 - val_accuracy: 0.8900\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting detection\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=0.5)\n",
    "\n",
    "\"\"\"\n",
    "You may have noticed that the validation loss, val_loss, was substantially higher than the training loss, loss. \n",
    "Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. \n",
    "When this happens, you will want to try decreasing the number of epochs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating models\n",
    "\n",
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "1/4 [======>.......................] - ETA: 0s - loss: 0.1738\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "4/4 [==============================] - 0s 2ms/step - loss: 0.1698\n",
    "    \n",
    "1/4 [======>.......................] - ETA: 0s - loss: 0.3251\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "4/4 [==============================] - 0s 1ms/step - loss: 0.2849\n",
    "    \n",
    "1/4 [======>.......................] - ETA: 0s - loss: 0.0425\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "4/4 [==============================] - 0s 2ms/step - loss: 0.0396\n",
    "    \n",
    "1/4 [======>.......................] - ETA: 0s - loss: 0.1414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
    "4/4 [==============================] - 0s 2ms/step - loss: 0.1454\n",
    "    \n",
    "     Small - Train: 0.16981548070907593, Test: 0.2848725914955139\n",
    "    Large - Train: 0.03957207128405571, Test: 0.14543527364730835\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing to train with Estimators\n",
    "\n",
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing['price'])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Estimators\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "# 2 nodes in both the first and second hidden\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)\n",
    "\n",
    "\"\"\"\n",
    "Note that you have other premade estimator options, such as BoostedTreesRegressor(), \n",
    "and can also create your own custom estimators.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
