{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Describing Keras\n",
    "Which of the following statements about Keras is false?\n",
    "\n",
    "--> Keras can work well on its own without using a backend, like TensorFlow.\n",
    "\n",
    "Note: Keras is a wrapper around a backend library, so a backend like TensorFlow, Theano, CNTK, etc must be provided.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Would you use deep learning?\n",
    "Imagine you're building an app that allows you to take a picture of your clothes and then shows you a pair of shoes \n",
    "that would match well. This app needs a machine learning module that's in charge of identifying the type of clothes \n",
    "you are wearing, as well as their color and texture. Would you use deep learning to accomplish this task?\n",
    "\n",
    "--> I'd use deep learning since we are dealing with unstructured data and neural networks work well with images.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello nets!\n",
    "\n",
    "# Import the Sequential model and Dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer and a hidden layer with 10 neurons\n",
    "model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "# Add a 1-neuron output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarise your model\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 10)                30        \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 11        \n",
    "=================================================================\n",
    "Total params: 41\n",
    "Trainable params: 41\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting parameters\n",
    "\n",
    "\"\"\"https://assets.datacamp.com/production/repositories/4335/datasets/9fd8a453d92bd8004c23ba415373c461f873913d/counting_parameters.png\"\"\"\n",
    "\n",
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 5)                 20        \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 6         \n",
    "=================================================================\n",
    "Total params: 26\n",
    "Trainable params: 26\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Given the model you just built, which answer is correct regarding the number of weights (parameters) in the hidden layer?\n",
    "\n",
    "--> There are 20 parameters, 15 from the connections of our inputs to our hidden layer \n",
    "and 5 from the bias weight of each neuron in the hidden layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build as shown!\n",
    "\n",
    "\"\"\"https://assets.datacamp.com/production/repositories/4335/datasets/cb59acc27b67d00078df48b5ec9d9c24744e50e9/build_as_shown.png\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Build the input and hidden layer\n",
    "model.add(Dense(3, input_shape=(2,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a model\n",
    "\n",
    "\"\"\"\n",
    "Your training data consist of measurements taken at time steps from -10 minutes before the impact region to +10 minutes after.\n",
    "Each time step can be viewed as an X coordinate in our graph, which has an associated position Y for the meteor orbit at that \n",
    "time step.\n",
    "\n",
    "https://assets.datacamp.com/production/repositories/4335/datasets/4f15cb3709395af69eee859c892c0775c610c46f/meteor_orbit_3.jpg\n",
    "\"\"\"\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "\"\"\"\n",
    "You are closer to forecasting the meteor orbit! It's important to note we aren't using an activation function \n",
    "in our output layer since y_positions aren't bounded and they can take any value. \n",
    "Your model is built to perform a regression task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps, y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the orbit!\n",
    "\n",
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Predict the eighty minute orbit\n",
    "eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "# Plot the eighty minute orbit \n",
    "plot_orbit(eighty_min_orbit)\n",
    "\n",
    "\"\"\"\n",
    "Your model fits perfectly to the scientists trajectory for time values between -10 to +10, \n",
    "the region where the meteor crosses the impact region, so we won't be hit! However, \n",
    "it starts to diverge when predicting for new values we haven't trained for. \n",
    "This shows neural networks learn according to the data they are fed with. \n",
    "Data quality and diversity are very important. You've barely scratched the surface of what neural networks can do. \n",
    "Are you prepared for the next chapter?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring dollar bills\n",
    "\n",
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use pairplot and set the hue to be our class column\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations per class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A binary classification model\n",
    "\n",
    "# Import the sequential model and dense layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 1)                 5         \n",
    "=================================================================\n",
    "Total params: 5\n",
    "Trainable params: 5\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this dollar bill fake ?\n",
    "\n",
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "\"\"\"\n",
    "Accuracy: 0.8252427167105443\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A multi-class model\n",
    "\n",
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes\n",
    "\n",
    "# Print the label encoded competitors\n",
    "print('Label encoded competitors: \\n',darts.competitor.head())\n",
    "\n",
    "\"\"\"\n",
    "Label encoded competitors: \n",
    " 0    2\n",
    "1    3\n",
    "2    1\n",
    "3    0\n",
    "4    2\n",
    "Name: competitor, dtype: int8\n",
    "\"\"\"\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)\n",
    "\n",
    "\"\"\"\n",
    "One-hot encoded competitors: \n",
    "     [[0. 0. 1. 0.]\n",
    "     [0. 0. 0. 1.]\n",
    "     [0. 1. 0. 0.]\n",
    "     ...\n",
    "     [0. 1. 0. 0.]\n",
    "     [0. 1. 0. 0.]\n",
    "     [0. 0. 0. 1.]]\n",
    "\n",
    "Each competitor is now a vector of length 4, full of zeroes except for the position representing her or himself.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on dart throwers\n",
    "\n",
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train, competitors_train, epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "\"\"\"\n",
    "Accuracy: 0.84375\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax predictions\n",
    "\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "\"\"\"\n",
    "Raw Model Predictions                         | True labels\n",
    "[0.34438723 0.00842557 0.63167274 0.01551455] | [0. 0. 1. 0.]\n",
    "[0.0989717  0.00530467 0.07537904 0.8203446 ] | [0. 0. 0. 1.]\n",
    "[0.33512568 0.00785374 0.28132284 0.37569773] | [0. 0. 0. 1.]\n",
    "[0.8547263  0.01328656 0.11279515 0.01919206] | [1. 0. 0. 0.]\n",
    "[0.3540977  0.00867271 0.6223853  0.01484426] | [0. 0. 1. 0.]\n",
    "\"\"\"\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "\"\"\"\n",
    "Rounded Model Predictions | True labels\n",
    "                        2 | [0. 0. 1. 0.]\n",
    "                        3 | [0. 0. 0. 1.]\n",
    "                        3 | [0. 0. 0. 1.]\n",
    "                        0 | [1. 0. 0. 0.]\n",
    "                        2 | [0. 0. 1. 0.]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An irrigation machine\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# Compile your model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss = 'binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 64)                1344      \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 3)                 195       \n",
    "=================================================================\n",
    "Total params: 1,539\n",
    "Trainable params: 1,539\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with multiple labels\n",
    "\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Predict on sensors_test and round up the predictions\n",
    "preds = model.predict(sensors_test)\n",
    "preds_rounded = np.round(preds)\n",
    "\n",
    "# Print rounded preds\n",
    "print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "# Evaluate your model's accuracy on the test data\n",
    "accuracy = model.evaluate(sensors_test, parcels_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "\"\"\"\n",
    "Accuracy: 0.9061111235618591\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history callback\n",
    "\n",
    "# Train your model and save its history\n",
    "h_callback = model.fit(X_train, y_train, epochs = 50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['acc'], h_callback.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping your model\n",
    "\n",
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test, y_test),\n",
    "           callbacks= [monitor_val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combination of callbacks\n",
    "\n",
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
    "                    validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Your Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning the digits\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))\n",
    "\n",
    "\"\"\"\n",
    "[[1.57801419e-01 3.13342916e-08 1.17609663e-04 ... 2.88670161e-03\n",
    "      1.75133277e-08 9.27261251e-04]\n",
    "     [9.17966962e-01 4.87130869e-08 1.09600009e-08 ... 1.81080788e-04\n",
    "      8.53955407e-06 9.01037129e-05]\n",
    "     [9.99938369e-01 1.82372684e-09 9.08111347e-12 ... 2.19022222e-05\n",
    "      2.59289088e-15 4.20937489e-08]\n",
    "     ...\n",
    "     [5.37219822e-01 5.52924506e-09 1.57055577e-10 ... 1.38584892e-05\n",
    "      4.47214532e-09 1.09312405e-05]\n",
    "     [2.70578653e-01 5.34917831e-07 8.48428527e-08 ... 1.55824000e-05\n",
    "      4.48651798e-03 3.27920467e-02]\n",
    "     [4.90147155e-03 2.87994535e-05 1.48348074e-04 ... 1.64761033e-04\n",
    "      2.08042213e-04 1.32970810e-01]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the model overfitting?\n",
    "\n",
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_test, y_test, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "\"\"\"\n",
    "Just by looking at the picture, do you think the learning curve shows this model is overfitting \n",
    "after having trained for 60 epochs?\n",
    "\n",
    "--> No, the test loss is not getting higher as the epochs go by.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need more data?\n",
    "\n",
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)\n",
    "\n",
    "\"\"\"\n",
    "The results shows that your model would not benefit a lot from more training data, since the test set accuracy is already starting to flatten. \n",
    "It's time to look at activation funtions!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.researchgate.net/profile/Junxi-Feng/publication/335845675/figure/fig3/AS:804124836765699@1568729709680/Commonly-used-activation-functions-a-Sigmoid-b-Tanh-c-ReLU-and-d-LReLU.ppm\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The sigmoid(),tanh(), ReLU(), and leaky_ReLU() functions have been defined and ready for you to use. Each function receives an input number X and returns its corresponding Y value.\n",
    "\n",
    "Which of the statements below is false?\n",
    "\n",
    "--> The sigmoid() and tanh() both take values close to -1 for big negative numbers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing activation functions\n",
    "\n",
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing activation functions II\n",
    "\n",
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "You've plotted both: loss and accuracy curves. It looks like sigmoid activation worked best for this particular model \n",
    "as the hidden layer's activation function. It led to a model with lower validation loss and higher accuracy after 100 epochs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing batch sizes\n",
    "\n",
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "\n",
    "\"\"\"\n",
    "The accuracy when using a batch of size 1 is:  0.9966666666666667\n",
    "\"\"\"\n",
    "##########################################################################\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))\n",
    "print(\"\\n The accuracy when using the whole training set as batch-size was: \",\n",
    "      model.evaluate(X_test, y_test)[1])\n",
    "\n",
    "\"\"\"\n",
    "The accuracy when using the whole training set as batch-size was:  0.553333334128062\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "You can see that accuracy is lower when using a batch size equal to the training set size. \n",
    "This is not because the network had more trouble learning the optimization function: \n",
    "Even though the same number of epochs were used for both batch sizes the number of resulting weight updates \n",
    "was very different!. With a batch of size the training set and 5 epochs we only get 5 updates total, \n",
    "each update computes and averaged gradient descent with all the training set observations. \n",
    "To obtain similar results with this batch size we should increase the number of epochs so that more weight updates take place.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalizing a familiar model\n",
    "\n",
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization effects\n",
    "\n",
    "# Train your standard model, storing its history callback\n",
    "h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history callback\n",
    "h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_histories_acc passing in both model histories\n",
    "compare_histories_acc(h1_callback, h2_callback)\n",
    "\n",
    "\"\"\"\n",
    "You can see that for this deep model batch normalization proved to be useful, \n",
    "helping the model obtain high accuracy values just over the first 10 training epochs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a model for tuning\n",
    "\n",
    "\"\"\"Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.\"\"\"\n",
    "\n",
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr = learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the model parameters\n",
    "\n",
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()\n",
    "\n",
    "\"\"\"\n",
    "Best: \n",
    "0.975395 using {learning_rate: 0.001, epochs: 50, batch_size: 128, activation: relu} \n",
    "Other: \n",
    "0.956063 (0.013236) with: {learning_rate: 0.1, epochs: 200, batch_size: 32, activation: tanh} \n",
    "0.970123 (0.019838) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: tanh} \n",
    "0.971880 (0.006524) with: {learning_rate: 0.01, epochs: 100, batch_size: 128, activation: tanh} \n",
    "0.724077 (0.072993) with: {learning_rate: 0.1, epochs: 50, batch_size: 32, activation: relu} \n",
    "0.588752 (0.281793) with: {learning_rate: 0.1, epochs: 100, batch_size: 256, activation: relu} \n",
    "0.966608 (0.004892) with: {learning_rate: 0.001, epochs: 100, batch_size: 128, activation: tanh} \n",
    "0.952548 (0.019734) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: relu} \n",
    "0.971880 (0.006524) with: {learning_rate: 0.001, epochs: 200, batch_size: 128, activation: relu}\n",
    "0.968366 (0.004239) with: {learning_rate: 0.01, epochs: 100, batch_size: 32, activation: relu}\n",
    "0.910369 (0.055824) with: {learning_rate: 0.1, epochs: 100, batch_size: 128, activation: relu}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with cross-validation\n",
    "\n",
    "\"\"\"Time to train your model with the best parameters found: 0.001 for the learning rate, \n",
    "50 epochs, a 128 batch_size and relu activations.\n",
    "\"\"\"\n",
    "\n",
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())\n",
    "\n",
    "\"\"\"\n",
    "The mean accuracy was: 0.9718834066666666\n",
    "With a standard deviation of: 0.002448915612216046\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Architectures\n",
    "It's time to get introduced to more advanced architectures! You will create an autoencoder to reconstruct noisy images, visualize convolutional neural network activations, use deep pre-trained models to classify images and learn more about recurrent neural networks and working with text as you build a network that predicts the next word in a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a flow of tensors\n",
    "\n",
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural separation\n",
    "\n",
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)\n",
    "    if i%4==0: \n",
    "      # Get the output of the first layer\n",
    "      layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "      # Evaluate model accuracy for this epoch\n",
    "      test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      plot()\n",
    "        \n",
    "\"\"\"\n",
    "That took a while! If you take a look at the graphs you can see how the neurons are learning to spread out\n",
    "the inputs based on whether they are fake or legit dollar bills. (A single fake dollar bill is represented as \n",
    "a purple dot in the graph) At the start the outputs are closer to each other, the weights are learned as epochs \n",
    "go by so that fake and legit dollar bills get a different, further and further apart output. \n",
    "Click in between the graphs fast, it's like a movie!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an autoencoder\n",
    "\n",
    "\"\"\"\n",
    "You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation\n",
    "of the image, which originally consists of 784 pixels (28 x 28). The autoencoder will essentially learn to turn the 784 pixels \n",
    "original image into a compressed 32 pixels image and learn how to use that encoded representation to bring back the original 784\n",
    "pixels image.\n",
    "\"\"\"\n",
    "\n",
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 32)                25120     \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 784)               25872     \n",
    "=================================================================\n",
    "Total params: 50,992\n",
    "Trainable params: 50,992\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-noising like an autoencoder\n",
    "\n",
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)\n",
    "\n",
    "\"\"\"Noisy vs decoded image\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a CNN model\n",
    "\n",
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from keras.layers import Flatten, Conv2D\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3\n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at convolutions\n",
    "\n",
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model's input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the activations of first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,19], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing your input image\n",
    "\n",
    "\"\"\"The original ResNet50 model was trained with images of size 224 x 224 pixels and a number of preprocessing operations; \n",
    "like the subtraction of the mean pixel value in the training set for all training images. \n",
    "You need to pre-process the images you want to predict on in the same way.\n",
    "\"\"\"\n",
    "\n",
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a real world model\n",
    "\n",
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "\n",
    "\"\"\"\n",
    "Predicted: [('n02088364', 'beagle', 0.8280003), ('n02089867', 'Walker_hound', 0.12915272), ('n02089973', 'English_foxhound', 0.03711732)]\n",
    "\n",
    "--> This dog is beagle!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text prediction with LSTMs\n",
    "\n",
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))\n",
    "\n",
    "\"\"\"\n",
    "Sentences: \n",
    "     ['it is not the', 'is not the strength', 'not the strength of', 'the strength of the', 'strength of the body'] \n",
    "     Sequences: \n",
    "     [[5, 2, 42, 1], [2, 42, 1, 6], [42, 1, 6, 4], [1, 6, 4, 1], [6, 4, 1, 10]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your LSTM model\n",
    "\n",
    "# Import the Embedding, LSTM and Dense layer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = vocab_size, input_length = 3, output_dim = 8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 3, 8)              352       \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 32)                5248      \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 32)                1056      \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 44)                1452      \n",
    "=================================================================\n",
    "Total params: 8,108\n",
    "Trainable params: 8,108\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "That's a nice looking model you've built! You'll see that this model is powerful enough to learn text relationships, \n",
    "we aren't using a lot of text in this tiny example and our sequences are quite short. \n",
    "This model is to be trained as usual, you would just need to compile it with an optimizer like adam and use crossentropy loss. \n",
    "This is because we have modeled this next word prediction task as a classification problem with all the unique words\n",
    "in our vocabulary as candidate classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode your predictions\n",
    "\n",
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function you just built, predict_text(), is ready to use. \n",
    "Remember that the model object is already passed by default as the second parameter so you \n",
    "just need to provide the function with your 3 word sentences.\n",
    "\n",
    "Try out these strings on your LSTM model:\n",
    "\n",
    "'meet revenge with'\n",
    "'the course of'\n",
    "'strength of the'\n",
    "Which sentence could be made with the word output from the sentences above?\n",
    "\n",
    "In [2]:\n",
    "predict_text('meet revenge with')\n",
    "Out[2]:'revenge'\n",
    "\n",
    "In [3]:\n",
    "predict_text('the course of')\n",
    "Out[3]:'history'\n",
    "\n",
    "In [4]:\n",
    "predict_text('strength of the')\n",
    "Out[4]:'spirit'\n",
    "\n",
    "revenge, history, spirit\n",
    "\n",
    "--> Revenge is your history and spirit\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
